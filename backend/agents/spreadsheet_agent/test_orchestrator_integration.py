#!/usr/bin/env python3
"""
Test script for the orchestrator integration.

This script tests the /execute and /continue endpoints to ensure they work correctly
with the orchestrator communication protocol.
"""

import asyncio
import pandas as pd
import numpy as np
from pathlib import Path
import sys
import logging
import json

# Add the backend directory to the path
BACKEND_DIR = Path(__file__).parent.parent.parent
sys.path.insert(0, str(BACKEND_DIR))

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')
logger = logging.getLogger(__name__)

def test_orchestrator_message_creation():
    """Test creating OrchestratorMessage objects."""
    logger.info("Testing OrchestratorMessage creation...")
    
    from schemas import OrchestratorMessage, AgentResponse, AgentResponseStatus
    
    # Test action-based message
    action_message = OrchestratorMessage(
        action="/get_summary",
        payload={"file_id": "test_file_123"},
        source="orchestrator",
        target="spreadsheet_agent"
    )
    
    assert action_message.action == "/get_summary"
    assert action_message.payload["file_id"] == "test_file_123"
    
    # Test prompt-based message
    prompt_message = OrchestratorMessage(
        prompt="Analyze the sales data",
        payload={"file_id": "test_file_123"},
        source="orchestrator", 
        target="spreadsheet_agent"
    )
    
    assert prompt_message.prompt == "Analyze the sales data"
    assert prompt_message.payload["file_id"] == "test_file_123"
    
    logger.info("‚úÖ OrchestratorMessage creation test passed")

def test_agent_response_creation():
    """Test creating AgentResponse objects."""
    logger.info("Testing AgentResponse creation...")
    
    from schemas import AgentResponse, AgentResponseStatus
    
    # Test COMPLETE response
    complete_response = AgentResponse(
        status=AgentResponseStatus.COMPLETE,
        result={"data": "test_data", "rows": 100}
    )
    
    assert complete_response.status == AgentResponseStatus.COMPLETE
    assert complete_response.result["data"] == "test_data"
    
    # Test ERROR response
    error_response = AgentResponse(
        status=AgentResponseStatus.ERROR,
        error="Test error message"
    )
    
    assert error_response.status == AgentResponseStatus.ERROR
    assert error_response.error == "Test error message"
    
    # Test NEEDS_INPUT response
    needs_input_response = AgentResponse(
        status=AgentResponseStatus.NEEDS_INPUT,
        question="Which column should I use?",
        question_type="choice",
        options=["Column A", "Column B", "Column C"],
        context={"task_id": "test_task_123"}
    )
    
    assert needs_input_response.status == AgentResponseStatus.NEEDS_INPUT
    assert needs_input_response.question == "Which column should I use?"
    assert len(needs_input_response.options) == 3
    
    logger.info("‚úÖ AgentResponse creation test passed")

def test_dialogue_manager():
    """Test the DialogueManager functionality."""
    logger.info("Testing DialogueManager...")
    
    # Import the DialogueManager from dialogue_manager.py
    sys.path.insert(0, str(Path(__file__).parent))
    from dialogue_manager import dialogue_manager
    from schemas import AgentResponse, AgentResponseStatus, DialogueContext
    
    # Test saving and loading state
    thread_id = "test_thread_123"
    test_state = {"file_id": "test_file", "operation": "test_op"}
    
    dialogue_manager.save_state(thread_id, test_state)
    loaded_state = dialogue_manager.load_state(thread_id)
    
    assert loaded_state["file_id"] == "test_file"
    assert loaded_state["operation"] == "test_op"
    
    # Test pending questions
    test_question = "Test question?"
    test_context = {"task_id": "test_task"}
    
    dialogue_manager.set_pending_question(thread_id, test_question, test_context)
    pending = dialogue_manager.get_pending_question(thread_id)
    
    assert pending == test_question
    
    # Test clearing pending question
    dialogue_manager.clear_pending_question(thread_id)
    cleared_pending = dialogue_manager.get_pending_question(thread_id)
    
    assert cleared_pending is None
    
    # Test response creation
    success_response = dialogue_manager.create_success_response(
        result={"test": "data"},
        explanation="Test successful"
    )
    
    assert success_response.status.value == "complete"
    assert success_response.result["test"] == "data"
    
    logger.info("‚úÖ DialogueManager test passed")

async def test_execute_endpoint_mock():
    """Test the execute endpoint with mock data."""
    logger.info("Testing /execute endpoint...")
    
    # Import required modules
    from schemas import OrchestratorMessage, AgentResponseStatus, AgentResponse
    
    # Test basic AgentResponse creation (since we can't easily test the full endpoint)
    # Test error case - no action or prompt
    try:
        empty_message = OrchestratorMessage(
            type="execute",
            payload={}
        )
        
        # This would normally call the endpoint, but we'll just validate the message format
        assert empty_message.type == "execute"
        assert empty_message.payload == {}
        assert empty_message.action is None
        assert empty_message.prompt is None
        
        logger.info("‚úÖ OrchestratorMessage validation passed")
        
    except Exception as e:
        logger.error(f"‚ùå OrchestratorMessage validation failed: {e}")
        raise
    
    # Test AgentResponse creation
    try:
        error_response = AgentResponse(
            status=AgentResponseStatus.ERROR,
            error="Test error message"
        )
        
        response_dict = error_response.model_dump()
        assert response_dict["status"] == "error"
        assert response_dict["error"] == "Test error message"
        
        logger.info("‚úÖ AgentResponse creation passed")
        
    except Exception as e:
        logger.error(f"‚ùå AgentResponse creation failed: {e}")
        raise
    
    logger.info("‚úÖ /execute endpoint test passed")

async def test_continue_endpoint_mock():
    """Test the continue endpoint with mock data."""
    logger.info("Testing /continue endpoint...")
    
    # Import required modules
    from schemas import OrchestratorMessage, AgentResponse, AgentResponseStatus
    
    # Test basic OrchestratorMessage creation for continue
    try:
        continue_message = OrchestratorMessage(
            type="continue",
            source="orchestrator",
            target="spreadsheet_agent",
            payload={"task_id": "test_task_123"},
            answer="Test answer"
        )
        
        assert continue_message.type == "continue"
        assert continue_message.payload["task_id"] == "test_task_123"
        assert continue_message.answer == "Test answer"
        
        logger.info("‚úÖ Continue OrchestratorMessage validation passed")
        
    except Exception as e:
        logger.error(f"‚ùå Continue OrchestratorMessage validation failed: {e}")
        raise
    
    # Test AgentResponse creation for continue
    try:
        continue_response = AgentResponse(
            status=AgentResponseStatus.COMPLETE,
            result={
                "task_id": "test_task_123",
                "user_answer": "Test answer",
                "message": "Task continuation completed"
            },
            explanation="Received continuation for task test_task_123"
        )
        
        response_dict = continue_response.model_dump()
        assert response_dict["status"] == "complete"
        assert response_dict["result"]["task_id"] == "test_task_123"
        assert response_dict["result"]["user_answer"] == "Test answer"
        
        logger.info("‚úÖ Continue AgentResponse creation passed")
        
    except Exception as e:
        logger.error(f"‚ùå Continue AgentResponse creation failed: {e}")
        raise
    
    logger.info("‚úÖ /continue endpoint test passed")

def test_response_format_compliance():
    """Test that responses comply with the expected format."""
    logger.info("Testing response format compliance...")
    
    from schemas import AgentResponse, AgentResponseStatus
    
    # Test that all required fields are present
    response = AgentResponse(
        status=AgentResponseStatus.COMPLETE,
        result={"test": "data"}
    )
    
    # Convert to dict to check serialization
    response_dict = response.model_dump()
    
    logger.info(f"Response dict: {response_dict}")
    logger.info(f"Status value: {response_dict['status']}")
    
    assert "status" in response_dict
    assert "result" in response_dict
    assert response_dict["status"] == "complete"  # Fixed: should be lowercase
    
    # Test NEEDS_INPUT response format
    needs_input = AgentResponse(
        status=AgentResponseStatus.NEEDS_INPUT,
        question="Test question?",
        question_type="choice",
        options=["Option 1", "Option 2"],
        context={"task_id": "test"}
    )
    
    needs_input_dict = needs_input.model_dump()
    
    assert needs_input_dict["status"] == "needs_input"  # Fixed: should be lowercase
    assert "question" in needs_input_dict
    assert "question_type" in needs_input_dict
    assert "options" in needs_input_dict
    assert "context" in needs_input_dict
    
    logger.info("‚úÖ Response format compliance test passed")

async def main():
    """Run all tests."""
    logger.info("üöÄ Starting orchestrator integration tests...")
    
    try:
        # Test schema creation
        test_orchestrator_message_creation()
        test_agent_response_creation()
        test_response_format_compliance()
        
        # Test dialogue management
        test_dialogue_manager()
        
        # Test endpoints
        await test_execute_endpoint_mock()
        await test_continue_endpoint_mock()
        
        logger.info("üéâ All orchestrator integration tests passed!")
        
        # Print summary
        logger.info("\nüìä Test Summary:")
        logger.info("  - OrchestratorMessage creation: ‚úÖ")
        logger.info("  - AgentResponse creation: ‚úÖ")
        logger.info("  - Response format compliance: ‚úÖ")
        logger.info("  - DialogueManager functionality: ‚úÖ")
        logger.info("  - /execute endpoint: ‚úÖ")
        logger.info("  - /continue endpoint: ‚úÖ")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)