"""
Orchestrator Comprehensive Test Runner

This module provides thorough end-to-end testing of the Orchestrator including:
- Basic chat functionality
- Tool invocation (TOOL:)
- Terminal commands (TERM:)  
- Agent delegation (AGENT:)
- Error handling
- Edge cases
- Multi-turn conversations
- Memory persistence

Usage:
    python orchestrator_test_runner.py              # Run all enabled tests
    python orchestrator_test_runner.py --category basic_chat  # Run specific category
    python orchestrator_test_runner.py --only-failed          # Re-run failed tests
"""

import asyncio
import os
import sys
import json
import logging
from datetime import datetime
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, field, asdict
from enum import Enum
import traceback
import io

# Force UTF-8 for Windows console
if sys.platform == "win32":
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# Add backend to path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from dotenv import load_dotenv
load_dotenv()

# =============================================================================
# DATA STRUCTURES
# =============================================================================

class TestStatus(str, Enum):
    PENDING = "pending"
    RUNNING = "running"
    PASSED = "passed"
    FAILED = "failed"
    SKIPPED = "skipped"
    ERROR = "error"

@dataclass
class TestCase:
    """A single test case"""
    id: str
    category: str
    name: str
    prompt: str
    expected_behavior: str
    validation_keywords: List[str] = field(default_factory=list)
    follow_up_prompt: Optional[str] = None  # For multi-turn tests
    requires_agents: List[str] = field(default_factory=list)  # Agents that must be running
    requires_tools: List[str] = field(default_factory=list)  # Tools that must exist
    status: TestStatus = TestStatus.PENDING
    result: Optional[Dict] = None
    error_message: Optional[str] = None
    duration_ms: float = 0
    log_file: Optional[str] = None

@dataclass
class TestRun:
    """Results of a complete test run"""
    run_id: str
    timestamp: str
    tests: List[TestCase] = field(default_factory=list)
    passed: int = 0
    failed: int = 0
    skipped: int = 0
    errors: int = 0

# =============================================================================
# TEST DEFINITIONS
# =============================================================================

def get_all_test_cases() -> List[TestCase]:
    """Define all test cases"""
    return [
        # =====================================================================
        # BASIC CHAT TESTS
        # =====================================================================
        TestCase(
            id="BC-01",
            category="basic_chat",
            name="Simple Greeting",
            prompt="Hello!",
            expected_behavior="Returns a friendly greeting without complex processing",
            validation_keywords=["hello", "hi", "greet", "help"]
        ),
        TestCase(
            id="BC-02",
            category="basic_chat",
            name="Capability Query",
            prompt="What can you do?",
            expected_behavior="Lists agent capabilities and available functions",
            validation_keywords=["help", "able", "can", "assist", "tool", "agent"]
        ),
        TestCase(
            id="BC-03",
            category="basic_chat",
            name="Thank You Response",
            prompt="Thank you very much!",
            expected_behavior="Acknowledges the gratitude politely",
            validation_keywords=["welcome", "glad", "happy", "help"]
        ),
        
        # =====================================================================
        # TOOL INVOCATION TESTS
        # =====================================================================
        TestCase(
            id="TI-01",
            category="tool_invocation",
            name="Wikipedia Lookup",
            prompt="Who is Albert Einstein? Use Wikipedia.",
            expected_behavior="Uses wiki_tools to retrieve Einstein biography",
            validation_keywords=["einstein", "physicist", "relativity", "theory", "nobel"],
            requires_tools=["get_wikipedia_summary"]
        ),
        TestCase(
            id="TI-02",
            category="tool_invocation",
            name="Web Search",
            prompt="Search the web for latest developments in AI in 2025",
            expected_behavior="Uses search_tools to find recent AI news",
            validation_keywords=["ai", "artificial", "intelligence", "2025"],
            requires_tools=["web_search_and_summarize"]
        ),
        TestCase(
            id="TI-03",
            category="tool_invocation",
            name="Stock Price Query",
            prompt="What is the current stock price of AAPL?",
            expected_behavior="Uses finance_tools to get Apple stock data",
            validation_keywords=["aapl", "apple", "price", "stock", "$"],
            requires_tools=["get_stock_price"]
        ),
        TestCase(
            id="TI-04",
            category="tool_invocation",
            name="News Fetch",
            prompt="Get the latest technology news headlines",
            expected_behavior="Uses news_tools to fetch recent tech news",
            validation_keywords=["news", "tech", "headline"],
            requires_tools=["get_top_headlines"]
        ),
        
        # =====================================================================
        # TERMINAL COMMAND TESTS
        # =====================================================================
        TestCase(
            id="TC-01",
            category="terminal_commands",
            name="File Creation",
            prompt="Create a file named 'test_orchestrator_output.txt' in the storage folder with content 'Orchestrator Test Successful'",
            expected_behavior="Uses TERM: to create file via echo/write command",
            validation_keywords=["created", "file", "success", "written"]
        ),
        TestCase(
            id="TC-02",
            category="terminal_commands",
            name="Directory Listing",
            prompt="List all files in the storage folder",
            expected_behavior="Uses TERM: with dir/ls to list directory contents",
            validation_keywords=["dir", "file", "folder"]
        ),
        TestCase(
            id="TC-03",
            category="terminal_commands",
            name="Echo Command",
            prompt="Echo 'Hello from Orchestrator' to the terminal",
            expected_behavior="Uses TERM: with echo command",
            validation_keywords=["hello", "orchestrator", "echo"]
        ),
        TestCase(
            id="TC-04",
            category="terminal_commands",
            name="Current Directory",
            prompt="Show me the current working directory",
            expected_behavior="Uses TERM: with pwd or cd to show directory",
            validation_keywords=["directory", "path", "orbimesh", "backend"]
        ),
        
        # =====================================================================
        # AGENT DELEGATION TESTS
        # =====================================================================
        TestCase(
            id="AD-01",
            category="agent_delegation",
            name="Spreadsheet Agent Health",
            prompt="Check if the SpreadsheetAgent is available and healthy",
            expected_behavior="Attempts to connect to SpreadsheetAgent",
            validation_keywords=["spreadsheet", "agent", "available", "health"],
            requires_agents=["SpreadsheetAgent"]
        ),
        TestCase(
            id="AD-02",
            category="agent_delegation",
            name="Document Agent Query",
            prompt="Use the DocumentAgent to describe its capabilities",
            expected_behavior="Delegates to DocumentAgent for capability info",
            validation_keywords=["document", "agent", "capability"],
            requires_agents=["DocumentAgent"]
        ),
        
        # =====================================================================
        # ERROR HANDLING TESTS
        # =====================================================================
        TestCase(
            id="EH-01",
            category="error_handling",
            name="Invalid Agent",
            prompt="Use the NonExistentFakeAgent to process something",
            expected_behavior="Gracefully handles agent not found error",
            validation_keywords=["not found", "unavailable", "error", "cannot"]
        ),
        TestCase(
            id="EH-02",
            category="error_handling",
            name="Missing File",
            prompt="Read the file 'this_file_definitely_does_not_exist_xyz123.txt'",
            expected_behavior="Gracefully handles file not found error",
            validation_keywords=["not found", "does not exist", "error", "cannot"]
        ),
        TestCase(
            id="EH-03",
            category="error_handling",
            name="Malformed Tool Args",
            prompt="Use the wiki tool with invalid args: {{{broken json",
            expected_behavior="Handles malformed input gracefully",
            validation_keywords=["error", "invalid", "failed", "cannot"]
        ),
        
        # =====================================================================
        # EDGE CASE TESTS
        # =====================================================================
        TestCase(
            id="EC-01",
            category="edge_cases",
            name="Empty Prompt",
            prompt="",
            expected_behavior="Handles empty prompt gracefully",
            validation_keywords=[]  # Any response is OK as long as no crash
        ),
        TestCase(
            id="EC-02",
            category="edge_cases",
            name="Unicode Characters",
            prompt="Translate 'hello' to Chinese and Japanese: ‰Ω†Â•Ω „Åì„Çì„Å´„Å°„ÅØ üéâ",
            expected_behavior="Handles Unicode characters without crashing",
            validation_keywords=["hello", "translate", "chinese", "japanese"]
        ),
        TestCase(
            id="EC-03",
            category="edge_cases",
            name="Special Characters",
            prompt="Calculate: 5 + 5 = ? and print the result with symbols: !@#$%^&*()",
            expected_behavior="Handles special characters",
            validation_keywords=["10", "result", "calculate"]
        ),
        TestCase(
            id="EC-04",
            category="edge_cases",
            name="Very Long Prompt",
            prompt="Please perform the following analysis: " + ("Analyze this data point. " * 100),
            expected_behavior="Handles long prompts without crashing",
            validation_keywords=["analyz", "data"]
        ),
        
        # =====================================================================
        # MULTI-TURN TESTS
        # =====================================================================
        TestCase(
            id="MT-01",
            category="multi_turn",
            name="Memory Recall",
            prompt="Remember that my favorite color is blue",
            expected_behavior="Stores information in memory",
            validation_keywords=["remember", "blue", "noted", "stored"],
            follow_up_prompt="What is my favorite color?"
        ),
        TestCase(
            id="MT-02",
            category="multi_turn",
            name="Context Continuity",
            prompt="My name is TestUser123",
            expected_behavior="Acknowledges the name",
            validation_keywords=["testuser", "name", "noted"],
            follow_up_prompt="What is my name?"
        ),
        
        # =====================================================================
        # MEMORY PERSISTENCE TESTS
        # =====================================================================
        TestCase(
            id="MP-01",
            category="memory_persistence",
            name="Task Completion Tracking",
            prompt="Add a note to my memory: 'Meeting at 3pm tomorrow'",
            expected_behavior="Stores note in persistent memory",
            validation_keywords=["noted", "stored", "memory", "meeting"]
        ),
    ]

# =============================================================================
# TEST RUNNER
# =============================================================================

class OrchestratorTestRunner:
    """Main test runner class"""
    
    def __init__(self, config_path: str = None):
        self.config = self._load_config(config_path)
        self.logger = self._setup_logging()
        self.run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.log_dir = os.path.join(
            os.path.dirname(__file__), 
            '../../logs/orchestrator_tests'
        )
        os.makedirs(self.log_dir, exist_ok=True)
        
        # Log file paths
        self.main_log_file = os.path.join(self.log_dir, f"test_run_{self.run_id}.log")
        self.results_file = os.path.join(self.log_dir, f"test_results_{self.run_id}.json")
        self.failed_tests_file = os.path.join(self.log_dir, "failed_tests.json")
        
    def _load_config(self, config_path: str = None) -> Dict:
        """Load test configuration"""
        if config_path is None:
            config_path = os.path.join(os.path.dirname(__file__), 'test_config.json')
        
        try:
            with open(config_path, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            self.logger.warning(f"Config not found at {config_path}, using defaults")
            return {
                "tests_to_run": {cat: True for cat in [
                    "basic_chat", "tool_invocation", "terminal_commands",
                    "agent_delegation", "error_handling", "edge_cases",
                    "multi_turn", "memory_persistence"
                ]},
                "settings": {
                    "max_iterations": 15,
                    "timeout_seconds": 120,
                    "verbose_logging": True
                }
            }
    
    def _setup_logging(self) -> logging.Logger:
        """Setup logging configuration"""
        logger = logging.getLogger("OrchestratorTestRunner")
        logger.setLevel(logging.DEBUG)
        
        # Console handler
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        console_format = logging.Formatter('%(asctime)s | %(levelname)s | %(message)s')
        console_handler.setFormatter(console_format)
        logger.addHandler(console_handler)
        
        return logger
    
    def _add_file_handler(self):
        """Add file handler after log dir is ready"""
        file_handler = logging.FileHandler(self.main_log_file, encoding='utf-8')
        file_handler.setLevel(logging.DEBUG)
        file_format = logging.Formatter('%(asctime)s | %(levelname)s | %(name)s | %(message)s')
        file_handler.setFormatter(file_format)
        self.logger.addHandler(file_handler)
    
    def log(self, message: str, level: str = "info"):
        """Log with file writing"""
        getattr(self.logger, level)(message)
        
        # Also write to main log file directly for guaranteed capture
        try:
            with open(self.main_log_file, 'a', encoding='utf-8') as f:
                timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                f.write(f"{timestamp} | {level.upper()} | {message}\n")
        except Exception:
            pass
    
    async def run_single_test(self, test: TestCase, graph, thread_id: str) -> TestCase:
        """Run a single test case and capture all output"""
        import time
        
        self.log(f"\n{'='*60}")
        self.log(f"TEST: {test.id} - {test.name}")
        self.log(f"Category: {test.category}")
        self.log(f"Prompt: {test.prompt[:100]}...")
        self.log(f"Expected: {test.expected_behavior}")
        self.log(f"{'='*60}")
        
        test.status = TestStatus.RUNNING
        start_time = time.time()
        
        try:
            # Build initial state
            initial_state = {
                "original_prompt": test.prompt,
                "todo_list": [],
                "memory": {},
                "iteration_count": 0,
                "max_iterations": self.config["settings"]["max_iterations"],
                "final_response": None,
                "current_task_id": None,
                "error": None
            }
            
            config = {
                "configurable": {"thread_id": thread_id},
                "recursion_limit": self.config["settings"]["max_iterations"]
            }
            
            # Run the graph
            self.log("Starting graph execution...")
            final_state = None
            iteration = 0
            
            async for output in graph.astream(initial_state, config):
                iteration += 1
                for node, state in output.items():
                    self.log(f"  Step {iteration}: Node=[{node}]")
                    
                    if state:
                        # Log todo list status
                        todo_list = state.get("todo_list", [])
                        if todo_list:
                            for t in todo_list:
                                self.log(f"    Task [{t.get('status', 'unknown')}]: {t.get('description', 'N/A')[:50]}")
                                if t.get('result'):
                                    self.log(f"      Result: {str(t.get('result'))[:200]}")
                        
                        # Log errors
                        if state.get("error"):
                            self.log(f"    ERROR: {state.get('error')}", "error")
                        
                        # Log final response
                        if state.get("final_response"):
                            self.log(f"    FINAL: {state.get('final_response')[:500]}")
                            final_state = state
                            break
                        
                        final_state = state
            
            # Capture results
            test.result = {
                "final_response": final_state.get("final_response") if final_state else None,
                "todo_list": final_state.get("todo_list") if final_state else [],
                "memory": final_state.get("memory") if final_state else {},
                "error": final_state.get("error") if final_state else None,
                "iterations": iteration
            }
            
            # Determine pass/fail based on results
            test = self._evaluate_test(test)
            
            # Handle follow-up for multi-turn tests
            if test.follow_up_prompt and test.status == TestStatus.PASSED:
                self.log(f"\n--- Follow-up Turn ---")
                self.log(f"Follow-up Prompt: {test.follow_up_prompt}")
                
                follow_up_state = {
                    "original_prompt": test.follow_up_prompt,
                    "todo_list": [],
                    "memory": final_state.get("memory", {}) if final_state else {},
                    "iteration_count": 0,
                    "max_iterations": self.config["settings"]["max_iterations"],
                    "final_response": None,
                    "current_task_id": None,
                    "error": None
                }
                
                follow_up_thread = f"{thread_id}_followup"
                follow_config = {
                    "configurable": {"thread_id": follow_up_thread},
                    "recursion_limit": self.config["settings"]["max_iterations"]
                }
                
                async for output in graph.astream(follow_up_state, follow_config):
                    for node, state in output.items():
                        if state and state.get("final_response"):
                            self.log(f"  Follow-up FINAL: {state.get('final_response')[:500]}")
                            test.result["follow_up_response"] = state.get("final_response")
                            break
            
        except asyncio.TimeoutError:
            test.status = TestStatus.ERROR
            test.error_message = "Test timed out"
            self.log(f"TIMEOUT: Test exceeded time limit", "error")
            
        except Exception as e:
            test.status = TestStatus.ERROR
            test.error_message = str(e)
            self.log(f"EXCEPTION: {e}", "error")
            self.log(traceback.format_exc(), "error")
        
        test.duration_ms = (time.time() - start_time) * 1000
        self.log(f"\nResult: {test.status.value} (Duration: {test.duration_ms:.0f}ms)")
        
        return test
    
    def _evaluate_test(self, test: TestCase) -> TestCase:
        """Evaluate test results based on validation criteria"""
        if not test.result:
            test.status = TestStatus.FAILED
            test.error_message = "No result captured"
            return test
        
        # Check for errors
        if test.result.get("error"):
            # For error handling tests, errors might be expected
            if test.category == "error_handling":
                # Error tests pass if they handle gracefully (have a final response)
                if test.result.get("final_response"):
                    test.status = TestStatus.PASSED
                else:
                    test.status = TestStatus.FAILED
                    test.error_message = f"Error not handled gracefully: {test.result.get('error')}"
            else:
                test.status = TestStatus.FAILED
                test.error_message = f"Unexpected error: {test.result.get('error')}"
            return test
        
        # Build combined text from ALL output sources for keyword checking
        # Priority: final_response, then all task results, then memory
        searchable_text = ""
        
        final_response = test.result.get("final_response", "")
        if final_response:
            searchable_text += str(final_response) + " "
        
        # Extract all task results from todo_list
        todo_list = test.result.get("todo_list", [])
        for task in todo_list:
            result = task.get("result")
            if result:
                # Handle both dict and string results
                if isinstance(result, dict):
                    searchable_text += json.dumps(result) + " "
                else:
                    searchable_text += str(result) + " "
            # Also check error/description for context
            if task.get("description"):
                searchable_text += str(task.get("description")) + " "
        
        # Add memory content
        memory = test.result.get("memory", {})
        if memory:
            searchable_text += json.dumps(memory) + " "
        
        # Edge case: empty prompt - just check no crash
        if test.category == "edge_cases" and not test.prompt:
            test.status = TestStatus.PASSED
            return test
        
        # Check if there's any meaningful output first
        if not searchable_text.strip() and test.category != "edge_cases":
            test.status = TestStatus.FAILED
            test.error_message = "No final response or task results generated"
            return test
        
        # Check validation keywords in combined searchable text
        if test.validation_keywords:
            searchable_lower = searchable_text.lower()
            found_keywords = [kw for kw in test.validation_keywords if kw.lower() in searchable_lower]
            
            self.log(f"  Validation: Searching for keywords in combined output ({len(searchable_text)} chars)")
            self.log(f"    Looking for: {test.validation_keywords}")
            self.log(f"    Found: {found_keywords}")
            
            if len(found_keywords) >= 1:  # At least one keyword found
                test.status = TestStatus.PASSED
            else:
                test.status = TestStatus.FAILED
                test.error_message = f"Expected keywords not found. Looking for: {test.validation_keywords}"
        else:
            # No keywords specified - pass if we got any output
            if searchable_text.strip():
                test.status = TestStatus.PASSED
            else:
                test.status = TestStatus.FAILED
                test.error_message = "No meaningful output"
        
        return test
    
    async def run_all_tests(self, category_filter: str = None, only_failed: bool = False) -> TestRun:
        """Run all enabled tests"""
        from orchestrator.graph import create_graph_with_checkpointer
        from langgraph.checkpoint.memory import MemorySaver
        
        self._add_file_handler()
        
        self.log(f"\n{'#'*60}")
        self.log(f"# ORCHESTRATOR TEST RUN: {self.run_id}")
        self.log(f"# Started: {datetime.now().isoformat()}")
        self.log(f"{'#'*60}\n")
        
        # Get test cases
        all_tests = get_all_test_cases()
        
        # Filter by category
        if category_filter:
            all_tests = [t for t in all_tests if t.category == category_filter]
            self.log(f"Filtered to category: {category_filter}")
        
        # Filter by enabled categories from config
        enabled_categories = [cat for cat, enabled in self.config["tests_to_run"].items() if enabled]
        all_tests = [t for t in all_tests if t.category in enabled_categories]
        self.log(f"Enabled categories: {enabled_categories}")
        
        # Filter to only failed tests if requested
        if only_failed and os.path.exists(self.failed_tests_file):
            with open(self.failed_tests_file, 'r') as f:
                failed_ids = json.load(f)
            all_tests = [t for t in all_tests if t.id in failed_ids]
            self.log(f"Running only failed tests: {failed_ids}")
        
        self.log(f"Total tests to run: {len(all_tests)}")
        
        # Create graph
        checkpointer = MemorySaver()
        graph = create_graph_with_checkpointer(checkpointer)
        
        # Run tests
        test_run = TestRun(
            run_id=self.run_id,
            timestamp=datetime.now().isoformat()
        )
        
        for i, test in enumerate(all_tests):
            self.log(f"\n[{i+1}/{len(all_tests)}] Running test {test.id}...")
            thread_id = f"test_{self.run_id}_{test.id}"
            
            try:
                test = await asyncio.wait_for(
                    self.run_single_test(test, graph, thread_id),
                    timeout=self.config["settings"]["timeout_seconds"]
                )
            except asyncio.TimeoutError:
                test.status = TestStatus.ERROR
                test.error_message = "Test timed out"
            
            test_run.tests.append(test)
            
            # Update counts
            if test.status == TestStatus.PASSED:
                test_run.passed += 1
            elif test.status == TestStatus.FAILED:
                test_run.failed += 1
            elif test.status == TestStatus.SKIPPED:
                test_run.skipped += 1
            elif test.status == TestStatus.ERROR:
                test_run.errors += 1
        
        # Save results
        self._save_results(test_run)
        
        # Print summary
        self._print_summary(test_run)
        
        return test_run
    
    def _save_results(self, test_run: TestRun):
        """Save test results to files"""
        # Full results
        results_data = {
            "run_id": test_run.run_id,
            "timestamp": test_run.timestamp,
            "summary": {
                "total": len(test_run.tests),
                "passed": test_run.passed,
                "failed": test_run.failed,
                "skipped": test_run.skipped,
                "errors": test_run.errors
            },
            "tests": []
        }
        
        failed_ids = []
        for test in test_run.tests:
            test_data = {
                "id": test.id,
                "category": test.category,
                "name": test.name,
                "status": test.status.value,
                "duration_ms": test.duration_ms,
                "error_message": test.error_message,
                "result": test.result
            }
            results_data["tests"].append(test_data)
            
            if test.status in [TestStatus.FAILED, TestStatus.ERROR]:
                failed_ids.append(test.id)
        
        # Save full results
        with open(self.results_file, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, default=str)
        
        # Save failed tests for re-run
        with open(self.failed_tests_file, 'w', encoding='utf-8') as f:
            json.dump(failed_ids, f)
        
        self.log(f"\nResults saved to: {self.results_file}")
        self.log(f"Failed tests saved to: {self.failed_tests_file}")
    
    def _print_summary(self, test_run: TestRun):
        """Print test run summary"""
        total = len(test_run.tests)
        
        print("\n" + "="*60)
        print("TEST RUN SUMMARY")
        print("="*60)
        print(f"Run ID: {test_run.run_id}")
        print(f"Total Tests: {total}")
        print(f"  ‚úÖ Passed:  {test_run.passed}")
        print(f"  ‚ùå Failed:  {test_run.failed}")
        print(f"  ‚ö†Ô∏è  Errors:  {test_run.errors}")
        print(f"  ‚è≠Ô∏è  Skipped: {test_run.skipped}")
        print(f"\nPass Rate: {(test_run.passed/total*100) if total > 0 else 0:.1f}%")
        
        # List failures
        if test_run.failed > 0 or test_run.errors > 0:
            print("\n‚ùå FAILED TESTS:")
            for test in test_run.tests:
                if test.status in [TestStatus.FAILED, TestStatus.ERROR]:
                    print(f"  - [{test.id}] {test.name}: {test.error_message}")
        
        print("\n" + "="*60)
        print(f"Log file: {self.main_log_file}")
        print("="*60)


# =============================================================================
# CLI ENTRY POINT
# =============================================================================

async def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="Orchestrator Test Runner")
    parser.add_argument("--category", type=str, help="Run only specific category")
    parser.add_argument("--only-failed", action="store_true", help="Re-run only failed tests")
    parser.add_argument("--config", type=str, help="Path to config file")
    args = parser.parse_args()
    
    runner = OrchestratorTestRunner(config_path=args.config)
    
    await runner.run_all_tests(
        category_filter=args.category,
        only_failed=args.only_failed
    )


if __name__ == "__main__":
    asyncio.run(main())
